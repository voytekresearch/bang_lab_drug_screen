{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Neural Activity Analysis for Multi-Electrode Array (MEA) Data\n",
    "\n",
    "This module processes spike train data from MEA plates, detecting bursts and\n",
    "analyzing neural activity patterns. Designed for drug screening experiments\n",
    "using electrophysiological recordings.\n",
    "\n",
    "Dependencies:\n",
    "    - numpy, pandas, matplotlib\n",
    "    - mat73 (for MATLAB v7.3 file loading)\n",
    "    - scipy\n",
    "    - pathlib, logging, json\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import os\n",
    "import json\n",
    "import mat73\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from scipy.signal import find_peaks, hilbert\n",
    "\n",
    "\n",
    "class MEADataProcessor:\n",
    "    \"\"\"\n",
    "    Processes Multi-Electrode Array (MEA) data for neural activity analysis.\n",
    "    \n",
    "    This class handles spike train analysis, burst detection, and visualization\n",
    "    for electrophysiological data stored in MATLAB format.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_path=None, output_subdir=\"processed_data\"):\n",
    "        \"\"\"\n",
    "        Initialize the MEA data processor.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        base_path : str or Path, optional\n",
    "            Base directory for data processing. If None, uses current directory.\n",
    "        output_subdir : str\n",
    "            Subdirectory name for output files\n",
    "        \"\"\"\n",
    "        self.base_path = Path(base_path) if base_path else Path.cwd()\n",
    "        self.output_dir = self.base_path / output_subdir\n",
    "        self.setup_logging()\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Set up logging configuration.\"\"\"\n",
    "        log_file = self.base_path / f'mea_processing_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def load_spike_data(self, filepath, well_row, well_col, channel_row, channel_col):\n",
    "        \"\"\"\n",
    "        Extract spike times from a single channel in a MEA plate.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str or Path\n",
    "            Path to the .mat file containing plate data\n",
    "        well_row, well_col : int\n",
    "            Row and column indices of the well (0-indexed)\n",
    "        channel_row, channel_col : int\n",
    "            Row and column indices of the channel within the well (0-indexed)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            DataFrame with 'Time (s)' column containing spike times\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data_dict = mat73.loadmat(str(filepath))\n",
    "            well = data_dict['Plate'][well_row][well_col]\n",
    "            channel = well[channel_row][channel_col]\n",
    "            \n",
    "            if not isinstance(channel, np.ndarray):\n",
    "                channel = np.array([])\n",
    "                self.logger.warning(f'Empty channel at ({channel_row}, {channel_col})')\n",
    "                \n",
    "            return pd.DataFrame({'Time (s)': channel})\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading spike data: {str(e)}\")\n",
    "            return pd.DataFrame({'Time (s)': np.array([])})\n",
    "\n",
    "    def detect_bursts(self, spiketrain, max_begin_isi=0.17, max_end_isi=0.3, \n",
    "                     min_ibi=0.2, min_burst_duration=0.01, min_spikes_in_burst=5):\n",
    "        \"\"\"\n",
    "        Detect bursts in spike train data using the MaxInterval method.\n",
    "        \n",
    "        This three-phase algorithm:\n",
    "        1. Identifies potential bursts based on inter-spike intervals (ISI)\n",
    "        2. Merges bursts separated by short inter-burst intervals (IBI)\n",
    "        3. Filters out bursts that don't meet duration/spike count criteria\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        spiketrain : array-like\n",
    "            Array of spike times in seconds\n",
    "        max_begin_isi : float\n",
    "            Maximum ISI to begin a burst (seconds)\n",
    "        max_end_isi : float\n",
    "            Maximum ISI to continue a burst (seconds)\n",
    "        min_ibi : float\n",
    "            Minimum inter-burst interval for separate bursts (seconds)\n",
    "        min_burst_duration : float\n",
    "            Minimum duration for a valid burst (seconds)\n",
    "        min_spikes_in_burst : int\n",
    "            Minimum number of spikes for a valid burst\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (burst_data_dict, num_rejected_bursts)\n",
    "            burst_data_dict: Dictionary with burst indices as keys, spike times as values\n",
    "            num_rejected_bursts: Number of bursts rejected in quality control\n",
    "        \"\"\"\n",
    "        if len(spiketrain) < 2:\n",
    "            return {}, 0\n",
    "            \n",
    "        all_burst_data = {}\n",
    "        \n",
    "        # Phase 1: Initial burst detection\n",
    "        in_burst = False\n",
    "        burst_num = 0\n",
    "        current_burst = []\n",
    "        \n",
    "        for n in range(1, len(spiketrain)):\n",
    "            isi = spiketrain[n] - spiketrain[n - 1]\n",
    "            \n",
    "            if in_burst:\n",
    "                if isi > max_end_isi:  # End the burst\n",
    "                    current_burst.append(spiketrain[n - 1])\n",
    "                    all_burst_data[burst_num] = np.array(current_burst)\n",
    "                    current_burst = []\n",
    "                    burst_num += 1\n",
    "                    in_burst = False\n",
    "                elif isi <= max_end_isi and n == len(spiketrain) - 1:\n",
    "                    current_burst.extend([spiketrain[n - 1], spiketrain[n]])\n",
    "                    all_burst_data[burst_num] = np.array(current_burst)\n",
    "                    burst_num += 1\n",
    "                else:\n",
    "                    current_burst.append(spiketrain[n - 1])\n",
    "            else:\n",
    "                if isi < max_begin_isi:\n",
    "                    current_burst.append(spiketrain[n - 1])\n",
    "                    in_burst = True\n",
    "        \n",
    "        if burst_num == 0:\n",
    "            return {}, 0\n",
    "            \n",
    "        # Calculate inter-burst intervals (IBIs)\n",
    "        ibi_list = []\n",
    "        for b in range(1, burst_num):\n",
    "            prev_burst_end = all_burst_data[b - 1][-1]\n",
    "            curr_burst_begin = all_burst_data[b][0]\n",
    "            ibi_list.append(curr_burst_begin - prev_burst_end)\n",
    "        \n",
    "        # Phase 2: Merge bursts with short IBIs\n",
    "        if len(ibi_list) > 0:\n",
    "            temp_bursts = all_burst_data.copy()\n",
    "            all_burst_data = {}\n",
    "            merged_burst_num = 0\n",
    "            \n",
    "            current_merged = temp_bursts[0]\n",
    "            \n",
    "            for b in range(1, len(temp_bursts)):\n",
    "                if b-1 < len(ibi_list) and ibi_list[b-1] < min_ibi:\n",
    "                    # Merge with previous burst\n",
    "                    current_merged = np.concatenate([current_merged, temp_bursts[b]])\n",
    "                else:\n",
    "                    # Save previous merged burst and start new one\n",
    "                    all_burst_data[merged_burst_num] = current_merged\n",
    "                    merged_burst_num += 1\n",
    "                    current_merged = temp_bursts[b]\n",
    "            \n",
    "            # Don't forget the last burst\n",
    "            all_burst_data[merged_burst_num] = current_merged\n",
    "        \n",
    "        # Phase 3: Quality control\n",
    "        temp_bursts = all_burst_data.copy()\n",
    "        all_burst_data = {}\n",
    "        final_burst_num = 0\n",
    "        rejected_count = 0\n",
    "        \n",
    "        for burst_data in temp_bursts.values():\n",
    "            burst_duration = burst_data[-1] - burst_data[0] if len(burst_data) > 0 else 0\n",
    "            \n",
    "            if (len(burst_data) >= min_spikes_in_burst and \n",
    "                burst_duration >= min_burst_duration):\n",
    "                all_burst_data[final_burst_num] = burst_data\n",
    "                final_burst_num += 1\n",
    "            else:\n",
    "                rejected_count += 1\n",
    "        \n",
    "        return all_burst_data, rejected_count\n",
    "\n",
    "    def analyze_well(self, filename, well_row, well_col, plot_spikes=False, plot_bursts=False):\n",
    "        \"\"\"\n",
    "        Analyze all channels in a single well of a MEA plate.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filename : str or Path\n",
    "            Path to the .mat file\n",
    "        well_row, well_col : int\n",
    "            Well coordinates (0-indexed)\n",
    "        plot_spikes : bool\n",
    "            Whether to create spike raster plots\n",
    "        plot_bursts : bool\n",
    "            Whether to highlight bursts in plots\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (channel_spikes, well_bursts, aggregated_spikes)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data_dict = mat73.loadmat(str(filename))\n",
    "            well = data_dict['Plate'][well_row][well_col]\n",
    "            \n",
    "            # Initialize storage\n",
    "            channel_spikes = {}\n",
    "            well_bursts = []\n",
    "            all_spikes = np.array([])\n",
    "            \n",
    "            # Process each channel (assuming 4x4 grid)\n",
    "            for i in range(4):\n",
    "                for j in range(4):\n",
    "                    channel_idx = i * 4 + j\n",
    "                    \n",
    "                    # Handle empty channels\n",
    "                    if not isinstance(well[i][j], np.ndarray):\n",
    "                        well[i][j] = np.array([])\n",
    "                        self.logger.warning(f'Empty channel at ({i}, {j}) in well ({well_row}, {well_col})')\n",
    "                    \n",
    "                    spike_times = well[i][j]\n",
    "                    if spike_times.ndim == 0:\n",
    "                        spike_times = np.array([spike_times]) if spike_times.size > 0 else np.array([])\n",
    "                    \n",
    "                    channel_spikes[channel_idx] = spike_times\n",
    "                    all_spikes = np.concatenate([all_spikes, spike_times]) if spike_times.size > 0 else all_spikes\n",
    "                    \n",
    "                    # Detect bursts for this channel\n",
    "                    if len(spike_times) > 1:\n",
    "                        burst_data, _ = self.detect_bursts(spike_times)\n",
    "                        well_bursts.extend(list(burst_data.values()))\n",
    "            \n",
    "            # Create visualizations if requested\n",
    "            if plot_spikes or plot_bursts:\n",
    "                self._create_well_visualization(well, well_row, well_col, all_spikes, \n",
    "                                              plot_spikes, plot_bursts)\n",
    "            \n",
    "            return list(channel_spikes.values()), well_bursts, all_spikes\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error analyzing well ({well_row}, {well_col}): {str(e)}\")\n",
    "            return [], [], np.array([])\n",
    "\n",
    "    def _create_well_visualization(self, well_data, well_row, well_col, all_spikes, \n",
    "                                 plot_spikes, plot_bursts):\n",
    "        \"\"\"Create visualization for a single well's activity.\"\"\"\n",
    "        fig, (ax_hist, ax_raster) = plt.subplots(\n",
    "            2, 1, figsize=(15, 8), \n",
    "            gridspec_kw={'height_ratios': [1, 2]}, \n",
    "            sharex=True\n",
    "        )\n",
    "        \n",
    "        channel_idx = 0\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                spike_times = well_data[i][j]\n",
    "                if spike_times.ndim == 0:\n",
    "                    spike_times = np.array([spike_times]) if spike_times.size > 0 else np.array([])\n",
    "                \n",
    "                # Plot bursts\n",
    "                if plot_bursts and len(spike_times) > 1:\n",
    "                    burst_data, _ = self.detect_bursts(spike_times)\n",
    "                    for burst in burst_data.values():\n",
    "                        if len(burst) > 0:\n",
    "                            burst_start = burst[0]\n",
    "                            burst_duration = burst[-1] - burst_start\n",
    "                            \n",
    "                            # Raster plot burst highlight\n",
    "                            raster_rect = Rectangle(\n",
    "                                (burst_start, channel_idx + 0.5), burst_duration, 1,\n",
    "                                alpha=0.5, edgecolor='turquoise', facecolor='turquoise'\n",
    "                            )\n",
    "                            ax_raster.add_patch(raster_rect)\n",
    "                            \n",
    "                            # Histogram burst highlight\n",
    "                            hist_rect = Rectangle(\n",
    "                                (burst_start, 0), burst_duration, 0.11,\n",
    "                                alpha=0.075, edgecolor='turquoise', facecolor='turquoise'\n",
    "                            )\n",
    "                            ax_hist.add_patch(hist_rect)\n",
    "                \n",
    "                # Plot spikes\n",
    "                if plot_spikes and len(spike_times) > 0:\n",
    "                    ax_raster.vlines(\n",
    "                        spike_times, channel_idx + 0.55, channel_idx + 1.45,\n",
    "                        linewidth=0.4, color='black', alpha=0.4\n",
    "                    )\n",
    "                \n",
    "                channel_idx += 1\n",
    "        \n",
    "        if plot_spikes and len(all_spikes) > 0:\n",
    "            ax_hist.hist(all_spikes, bins=min(1200, len(all_spikes)), \n",
    "                        density=True, color='black')\n",
    "            ax_hist.set_ylabel('Frequency')\n",
    "            ax_hist.set_title(f'Neural Activity Summary - Well [{well_row}, {well_col}]')\n",
    "            \n",
    "            ax_raster.set_yticks(range(1, 17))\n",
    "            ax_raster.set_ylabel('Channels')\n",
    "            ax_raster.set_xlabel('Time (s)')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    def process_plate_file(self, filename, condition_name=None):\n",
    "        \"\"\"\n",
    "        Process an entire plate file (all wells).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filename : str or Path\n",
    "            Path to the .mat file\n",
    "        condition_name : str, optional\n",
    "            Experimental condition name for labeling\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Results dictionary with well data\n",
    "        \"\"\"\n",
    "        filename = Path(filename)\n",
    "        if condition_name is None:\n",
    "            condition_name = filename.stem\n",
    "            \n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            # Process standard 6x8 well plate\n",
    "            for row in range(6):\n",
    "                for col in range(8):\n",
    "                    well_id = f\"well_{row:02d}_{col:02d}_{condition_name}\"\n",
    "                    self.logger.info(f\"Processing {well_id}\")\n",
    "                    \n",
    "                    try:\n",
    "                        spikes, bursts, all_spikes = self.analyze_well(\n",
    "                            filename, row, col, plot_spikes=False, plot_bursts=False\n",
    "                        )\n",
    "                        \n",
    "                        results[well_id] = {\n",
    "                            \"channel_spikes\": spikes,\n",
    "                            \"detected_bursts\": bursts,\n",
    "                            \"aggregated_spikes\": all_spikes,\n",
    "                            \"metadata\": {\n",
    "                                \"well_position\": (row, col),\n",
    "                                \"condition\": condition_name,\n",
    "                                \"num_channels\": len(spikes),\n",
    "                                \"total_spikes\": len(all_spikes),\n",
    "                                \"num_bursts\": len(bursts)\n",
    "                            }\n",
    "                        }\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Error processing {well_id}: {str(e)}\")\n",
    "                        results[well_id] = {\"error\": str(e)}\n",
    "                        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing plate {filename}: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "        return results\n",
    "\n",
    "    def save_results(self, results, output_name):\n",
    "        \"\"\"\n",
    "        Save processing results to JSON files.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        results : dict\n",
    "            Results from process_plate_file\n",
    "        output_name : str\n",
    "            Base name for output files\n",
    "        \"\"\"\n",
    "        output_path = self.output_dir / output_name\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        class NumpyEncoder(json.JSONEncoder):\n",
    "            def default(self, obj):\n",
    "                if isinstance(obj, np.ndarray):\n",
    "                    return obj.tolist()\n",
    "                elif isinstance(obj, np.integer):\n",
    "                    return int(obj)\n",
    "                elif isinstance(obj, np.floating):\n",
    "                    return float(obj)\n",
    "                return super().default(obj)\n",
    "        \n",
    "        # Save individual well results\n",
    "        for well_id, data in results.items():\n",
    "            file_path = output_path / f\"{well_id}.json\"\n",
    "            try:\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump(data, f, cls=NumpyEncoder, indent=2)\n",
    "                self.logger.info(f\"Saved {well_id} to {file_path}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error saving {well_id}: {str(e)}\")\n",
    "        \n",
    "        # Save summary statistics\n",
    "        summary = self._generate_summary(results)\n",
    "        summary_path = output_path / \"processing_summary.json\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary, f, cls=NumpyEncoder, indent=2)\n",
    "\n",
    "    def _generate_summary(self, results):\n",
    "        \"\"\"Generate summary statistics from processing results.\"\"\"\n",
    "        summary = {\n",
    "            \"total_wells_processed\": len(results),\n",
    "            \"successful_wells\": sum(1 for r in results.values() if \"error\" not in r),\n",
    "            \"failed_wells\": sum(1 for r in results.values() if \"error\" in r),\n",
    "            \"total_spikes\": 0,\n",
    "            \"total_bursts\": 0,\n",
    "            \"processing_timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        for well_data in results.values():\n",
    "            if \"error\" not in well_data:\n",
    "                summary[\"total_spikes\"] += well_data[\"metadata\"][\"total_spikes\"]\n",
    "                summary[\"total_bursts\"] += well_data[\"metadata\"][\"num_bursts\"]\n",
    "        \n",
    "        return summary\n",
    "\n",
    "    def process_folder(self, folder_path):\n",
    "        \"\"\"\n",
    "        Process all .mat files in a specified folder.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        folder_path : str or Path\n",
    "            Path to folder containing .mat files\n",
    "        \"\"\"\n",
    "        folder_path = Path(folder_path)\n",
    "        mat_files = list(folder_path.glob('*.mat'))\n",
    "        \n",
    "        self.logger.info(f\"Found {len(mat_files)} .mat files in {folder_path}\")\n",
    "        \n",
    "        for file_path in mat_files:\n",
    "            try:\n",
    "                self.logger.info(f\"Processing {file_path.name}\")\n",
    "                results = self.process_plate_file(file_path)\n",
    "                self.save_results(results, file_path.stem)\n",
    "                self.logger.info(f\"Completed {file_path.name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to process {file_path.name}: {str(e)}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example usage of the MEADataProcessor.\n",
    "    \n",
    "    Modify the paths below to match your data organization.\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    base_directory = Path(\"your_data_directory_here\")  # Update this path\n",
    "    input_folder = base_directory / \"raw_data\"         # Folder with .mat files\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = MEADataProcessor(\n",
    "        base_path=base_directory,\n",
    "        output_subdir=\"processed_results\"\n",
    "    )\n",
    "    \n",
    "    # Process all files in the input folder\n",
    "    if input_folder.exists():\n",
    "        processor.process_folder(input_folder)\n",
    "    else:\n",
    "        print(f\"Input folder {input_folder} does not exist. Please update the path.\")\n",
    "        \n",
    "    # Alternative: Process a single file\n",
    "    # single_file = input_folder / \"your_plate_file.mat\"\n",
    "    # if single_file.exists():\n",
    "    #     results = processor.process_plate_file(single_file, \"control_condition\")\n",
    "    #     processor.save_results(results, \"single_plate_analysis\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "team_ndd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
