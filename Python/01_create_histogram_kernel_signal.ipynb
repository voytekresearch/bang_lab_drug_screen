{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Neural Oscillation Analysis Module\n",
    "\n",
    "This module processes spike train data to detect neural oscillations using spectral analysis.\n",
    "It converts discrete spike times into smoothed firing rate signals, computes power spectra,\n",
    "and uses the SpectralModel (FOOOF) algorithm to separate periodic oscillations from \n",
    "aperiodic background activity.\n",
    "\n",
    "Key Features:\n",
    "- Double exponential kernel smoothing for realistic neural dynamics\n",
    "- Welch's method for robust power spectral density estimation\n",
    "- Automated oscillation detection with parameterized fitting\n",
    "- Batch processing with comprehensive error handling\n",
    "\n",
    "Dependencies:\n",
    "    - numpy, scipy\n",
    "    - neurodsp (for spectral analysis)\n",
    "    - specparam (FOOOF algorithm)\n",
    "    - pathlib, json, pickle\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Union, Optional\n",
    "\n",
    "import numpy as np\n",
    "from neurodsp.spectral import compute_spectrum_welch\n",
    "from specparam import SpectralModel\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "class NeuralSpectralAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes neural oscillations in spike train data using spectral methods.\n",
    "    \n",
    "    This class processes spike timing data through several stages:\n",
    "    1. Converts spike times to smoothed firing rate using biophysical kernels\n",
    "    2. Computes power spectral density using Welch's method\n",
    "    3. Fits parametric models to separate oscillations from aperiodic activity\n",
    "    4. Saves results for further analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_path: Union[str, Path] = None,\n",
    "                 output_subdir: str = \"spectral_analysis\",\n",
    "                 bin_size_ms: float = 2.0,\n",
    "                 kernel_window_ms: int = 300,\n",
    "                 tau_rise_ms: float = 2.0,\n",
    "                 tau_decay_ms: float = 25.0):\n",
    "        \"\"\"\n",
    "        Initialize the spectral analyzer with processing parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        base_path : str or Path, optional\n",
    "            Base directory for data processing\n",
    "        output_subdir : str\n",
    "            Subdirectory name for output files\n",
    "        bin_size_ms : float\n",
    "            Temporal resolution for spike histogram (milliseconds)\n",
    "        kernel_window_ms : int\n",
    "            Duration of smoothing kernel (milliseconds)\n",
    "        tau_rise_ms : float\n",
    "            Rise time constant for double exponential kernel (milliseconds)\n",
    "        tau_decay_ms : float\n",
    "            Decay time constant for double exponential kernel (milliseconds)\n",
    "        \"\"\"\n",
    "        self.base_path = Path(base_path) if base_path else Path.cwd()\n",
    "        self.output_dir = self.base_path / output_subdir\n",
    "        \n",
    "        # Temporal processing parameters\n",
    "        self.bin_size_ms = float(bin_size_ms)\n",
    "        self.kernel_window_ms = int(kernel_window_ms)\n",
    "        self.tau_rise_ms = float(tau_rise_ms)\n",
    "        self.tau_decay_ms = float(tau_decay_ms)\n",
    "        \n",
    "        # Derived parameters\n",
    "        self.sampling_rate = 1000.0 / self.bin_size_ms  # Hz\n",
    "        \n",
    "        self.setup_logging()\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging for processing tracking.\"\"\"\n",
    "        log_file = self.base_path / f'spectral_analysis_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    @staticmethod\n",
    "    def double_exponential_kernel(t: np.ndarray, tau_rise: float, tau_decay: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create a double exponential kernel modeling glutamatergic synaptic dynamics.\n",
    "        \n",
    "        This kernel mimics the typical rise and decay phases of excitatory postsynaptic\n",
    "        potentials (EPSPs), providing biologically realistic smoothing of spike trains.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        t : np.ndarray\n",
    "            Time points for kernel evaluation (milliseconds)\n",
    "        tau_rise : float\n",
    "            Rise time constant (milliseconds)\n",
    "        tau_decay : float\n",
    "            Decay time constant (milliseconds)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Normalized kernel values\n",
    "        \"\"\"\n",
    "        kernel = (1.0 - np.exp(-t / tau_rise)) * np.exp(-t / tau_decay)\n",
    "        return kernel / np.sum(kernel)  # Normalize to preserve total spike count\n",
    "\n",
    "    def spikes_to_smoothed_signal(self, spike_times: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Convert discrete spike times to continuous smoothed firing rate signal.\n",
    "        \n",
    "        This method:\n",
    "        1. Creates a histogram of spike times at specified temporal resolution\n",
    "        2. Applies a double exponential kernel for biologically realistic smoothing\n",
    "        3. Returns time bins and corresponding smoothed firing rates\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        spike_times : np.ndarray\n",
    "            Array of spike times in seconds\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (time_bins, smoothed_signal) where:\n",
    "            - time_bins: Time points for each bin (seconds)\n",
    "            - smoothed_signal: Smoothed firing rate density\n",
    "        \"\"\"\n",
    "        if len(spike_times) == 0:\n",
    "            return np.array([]), np.array([])\n",
    "            \n",
    "        # Determine time range and create bins\n",
    "        max_time = np.max(spike_times)\n",
    "        num_bins = int(np.ceil((max_time * 1000) / self.bin_size_ms))\n",
    "        time_bins = np.linspace(0, max_time, num_bins + 1)\n",
    "        \n",
    "        # Create spike histogram\n",
    "        spike_histogram, _ = np.histogram(spike_times, bins=time_bins, density=True)\n",
    "        \n",
    "        # Generate smoothing kernel\n",
    "        kernel_time = np.linspace(0, self.kernel_window_ms, num=self.kernel_window_ms)\n",
    "        kernel = self.double_exponential_kernel(\n",
    "            kernel_time, self.tau_rise_ms, self.tau_decay_ms\n",
    "        )\n",
    "        \n",
    "        # Apply smoothing through convolution\n",
    "        smoothed_signal = np.convolve(spike_histogram, kernel, mode='same')\n",
    "        \n",
    "        return time_bins[:-1], smoothed_signal\n",
    "\n",
    "    def compute_power_spectrum(self, signal: np.ndarray, \n",
    "                             frequency_range: Tuple[float, float] = (0.05, 40.0),\n",
    "                             window_duration_sec: float = 60.0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute power spectral density using Welch's method.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : np.ndarray\n",
    "            Input signal for spectral analysis\n",
    "        frequency_range : tuple\n",
    "            (min_freq, max_freq) in Hz for analysis\n",
    "        window_duration_sec : float\n",
    "            Duration of each window for Welch's method (seconds)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (frequencies, power_spectrum)\n",
    "        \"\"\"\n",
    "        # Calculate Welch parameters\n",
    "        nperseg = int(self.sampling_rate * window_duration_sec)\n",
    "        noverlap = nperseg // 2\n",
    "        \n",
    "        frequencies, power_spectrum = compute_spectrum_welch(\n",
    "            signal, \n",
    "            self.sampling_rate,\n",
    "            avg_type='median',\n",
    "            window='hann',\n",
    "            nperseg=nperseg,\n",
    "            noverlap=noverlap,\n",
    "            f_range=frequency_range\n",
    "        )\n",
    "        \n",
    "        return frequencies, power_spectrum\n",
    "\n",
    "    def fit_spectral_model(self, frequencies: np.ndarray, power_spectrum: np.ndarray,\n",
    "                          fit_range: Tuple[float, float] = (0.5, 13.0)) -> SpectralModel:\n",
    "        \"\"\"\n",
    "        Fit parametric spectral model to separate oscillations from aperiodic activity.\n",
    "        \n",
    "        Uses the SpectralModel (FOOOF) algorithm to decompose power spectra into:\n",
    "        - Aperiodic component (1/f-like background)\n",
    "        - Periodic components (oscillatory peaks)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        frequencies : np.ndarray\n",
    "            Frequency values (Hz)\n",
    "        power_spectrum : np.ndarray\n",
    "            Power spectral density values\n",
    "        fit_range : tuple\n",
    "            (min_freq, max_freq) for model fitting\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        SpectralModel\n",
    "            Fitted model containing oscillation parameters\n",
    "        \"\"\"\n",
    "        spectral_model = SpectralModel(\n",
    "            peak_width_limits=[1, 8],      # Expected oscillation bandwidth (Hz)\n",
    "            min_peak_height=0.2,           # Minimum peak height above aperiodic\n",
    "            max_n_peaks=4,                 # Maximum number of oscillations to detect\n",
    "            peak_threshold=1.5,            # Statistical threshold for peak detection\n",
    "            aperiodic_mode='fixed'         # Use fixed aperiodic fitting\n",
    "        )\n",
    "        \n",
    "        spectral_model.fit(frequencies, power_spectrum, fit_range)\n",
    "        return spectral_model\n",
    "\n",
    "    def analyze_well_data(self, well_data: Dict,\n",
    "                         frequency_range: Tuple[float, float] = (0.05, 40.0),\n",
    "                         fit_range: Tuple[float, float] = (0.5, 13.0)) -> Dict:\n",
    "        \"\"\"\n",
    "        Complete spectral analysis pipeline for a single well.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        well_data : dict\n",
    "            Dictionary containing 'all_spikes' key with spike time array\n",
    "        frequency_range : tuple\n",
    "            Frequency range for power spectrum computation\n",
    "        fit_range : tuple\n",
    "            Frequency range for oscillation model fitting\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Comprehensive analysis results including:\n",
    "            - Smoothed signal and time bins\n",
    "            - Power spectrum and frequencies\n",
    "            - Fitted spectral model\n",
    "            - Processing metadata\n",
    "        \"\"\"\n",
    "        # Extract and validate spike data\n",
    "        spike_times = np.array(well_data.get('all_spikes', []))\n",
    "        \n",
    "        if len(spike_times) == 0:\n",
    "            self.logger.warning(\"No spikes found in well data\")\n",
    "            return self._create_empty_results()\n",
    "        \n",
    "        # Convert spikes to smoothed signal\n",
    "        time_bins, smoothed_signal = self.spikes_to_smoothed_signal(spike_times)\n",
    "        \n",
    "        if len(smoothed_signal) == 0:\n",
    "            self.logger.warning(\"Failed to create smoothed signal\")\n",
    "            return self._create_empty_results()\n",
    "        \n",
    "        # Compute power spectrum\n",
    "        frequencies, power_spectrum = self.compute_power_spectrum(\n",
    "            smoothed_signal, frequency_range\n",
    "        )\n",
    "        \n",
    "        # Fit spectral model for oscillation detection\n",
    "        spectral_model = self.fit_spectral_model(frequencies, power_spectrum, fit_range)\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            'time_bins': time_bins,\n",
    "            'smoothed_signal': smoothed_signal,\n",
    "            'frequencies': frequencies,\n",
    "            'power_spectrum': power_spectrum,\n",
    "            'spectral_model': spectral_model,\n",
    "            'processing_metadata': {\n",
    "                'sampling_rate_hz': self.sampling_rate,\n",
    "                'bin_size_ms': self.bin_size_ms,\n",
    "                'kernel_parameters': {\n",
    "                    'window_ms': self.kernel_window_ms,\n",
    "                    'tau_rise_ms': self.tau_rise_ms,\n",
    "                    'tau_decay_ms': self.tau_decay_ms\n",
    "                },\n",
    "                'spectral_parameters': {\n",
    "                    'frequency_range': frequency_range,\n",
    "                    'fit_range': fit_range\n",
    "                },\n",
    "                'n_spikes': len(spike_times),\n",
    "                'recording_duration_sec': float(np.max(spike_times)) if len(spike_times) > 0 else 0.0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _create_empty_results(self) -> Dict:\n",
    "        \"\"\"Create empty results structure for failed processing.\"\"\"\n",
    "        return {\n",
    "            'time_bins': np.array([]),\n",
    "            'smoothed_signal': np.array([]),\n",
    "            'frequencies': np.array([]),\n",
    "            'power_spectrum': np.array([]),\n",
    "            'spectral_model': None,\n",
    "            'processing_metadata': {\n",
    "                'error': 'No valid spike data found',\n",
    "                'sampling_rate_hz': self.sampling_rate\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def process_json_file(self, json_file_path: Union[str, Path]) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Process a single JSON file containing well data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        json_file_path : str or Path\n",
    "            Path to JSON file with spike data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict or None\n",
    "            Analysis results or None if processing failed\n",
    "        \"\"\"\n",
    "        json_file_path = Path(json_file_path)\n",
    "        \n",
    "        try:\n",
    "            with open(json_file_path, 'r') as f:\n",
    "                well_data = json.load(f)\n",
    "            \n",
    "            results = self.analyze_well_data(well_data)\n",
    "            self.logger.info(f\"Successfully processed {json_file_path.name}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing {json_file_path.name}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def save_results(self, results: Dict, output_file_path: Union[str, Path]):\n",
    "        \"\"\"\n",
    "        Save analysis results to pickle file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        results : dict\n",
    "            Analysis results from analyze_well_data\n",
    "        output_file_path : str or Path\n",
    "            Path for output pickle file\n",
    "        \"\"\"\n",
    "        output_file_path = Path(output_file_path)\n",
    "        output_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            with open(output_file_path, 'wb') as f:\n",
    "                pickle.dump(results, f)\n",
    "            self.logger.info(f\"Results saved to {output_file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving results to {output_file_path}: {str(e)}\")\n",
    "\n",
    "    def process_folder(self, input_folder: Union[str, Path], \n",
    "                      output_folder: Union[str, Path] = None) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Process all JSON files in a folder and save results as pickle files.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_folder : str or Path\n",
    "            Folder containing JSON files to process\n",
    "        output_folder : str or Path, optional\n",
    "            Folder for saving pickle files. If None, uses output_dir.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Processing statistics: {'processed': int, 'failed': int, 'errors': list}\n",
    "        \"\"\"\n",
    "        input_folder = Path(input_folder)\n",
    "        if output_folder is None:\n",
    "            output_folder = self.output_dir / input_folder.name\n",
    "        else:\n",
    "            output_folder = Path(output_folder)\n",
    "        \n",
    "        output_folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize processing statistics\n",
    "        stats = {'processed': 0, 'failed': 0, 'errors': []}\n",
    "        \n",
    "        # Process each JSON file\n",
    "        json_files = list(input_folder.glob('*.json'))\n",
    "        self.logger.info(f\"Found {len(json_files)} JSON files to process in {input_folder}\")\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            try:\n",
    "                # Process the file\n",
    "                results = self.process_json_file(json_file)\n",
    "                \n",
    "                if results is not None:\n",
    "                    # Save results as pickle file\n",
    "                    pickle_file = output_folder / f\"{json_file.stem}.pkl\"\n",
    "                    self.save_results(results, pickle_file)\n",
    "                    stats['processed'] += 1\n",
    "                else:\n",
    "                    stats['failed'] += 1\n",
    "                    stats['errors'].append({\n",
    "                        'file': str(json_file),\n",
    "                        'error': 'Processing returned None',\n",
    "                        'timestamp': datetime.now().isoformat()\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                stats['failed'] += 1\n",
    "                error_info = {\n",
    "                    'file': str(json_file),\n",
    "                    'error': str(e),\n",
    "                    'error_type': type(e).__name__,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                stats['errors'].append(error_info)\n",
    "                self.logger.error(f\"Failed to process {json_file.name}: {str(e)}\")\n",
    "        \n",
    "        # Save error log if there were failures\n",
    "        if stats['errors']:\n",
    "            error_log_path = output_folder / f\"processing_errors_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            with open(error_log_path, 'w') as f:\n",
    "                json.dump(stats['errors'], f, indent=2)\n",
    "            self.logger.info(f\"Error log saved to {error_log_path}\")\n",
    "        \n",
    "        self.logger.info(f\"Processing complete: {stats['processed']} successful, {stats['failed']} failed\")\n",
    "        return stats\n",
    "\n",
    "    def process_nested_folders(self, base_input_folder: Union[str, Path],\n",
    "                              base_output_folder: Union[str, Path] = None) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Process multiple folders containing JSON files.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        base_input_folder : str or Path\n",
    "            Base folder containing subfolders with JSON files\n",
    "        base_output_folder : str or Path, optional\n",
    "            Base folder for saving results\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Processing statistics for each subfolder\n",
    "        \"\"\"\n",
    "        base_input_folder = Path(base_input_folder)\n",
    "        if base_output_folder is None:\n",
    "            base_output_folder = self.output_dir\n",
    "        else:\n",
    "            base_output_folder = Path(base_output_folder)\n",
    "        \n",
    "        all_stats = {}\n",
    "        \n",
    "        # Process each subfolder\n",
    "        for subfolder in base_input_folder.iterdir():\n",
    "            if subfolder.is_dir():\n",
    "                self.logger.info(f\"Processing folder: {subfolder.name}\")\n",
    "                output_subfolder = base_output_folder / subfolder.name\n",
    "                \n",
    "                try:\n",
    "                    stats = self.process_folder(subfolder, output_subfolder)\n",
    "                    all_stats[subfolder.name] = stats\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing folder {subfolder.name}: {str(e)}\")\n",
    "                    all_stats[subfolder.name] = {\n",
    "                        'processed': 0, \n",
    "                        'failed': 0, \n",
    "                        'errors': [{'folder_error': str(e)}]\n",
    "                    }\n",
    "        \n",
    "        return all_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example usage of the NeuralSpectralAnalyzer.\n",
    "    \n",
    "    Configure the paths below to match your data organization.\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    base_directory = Path(\"your_data_directory_here\")  # Update this path\n",
    "    json_input_folder = base_directory / \"processed_spike_data\"  # Folder with JSON files\n",
    "    \n",
    "    # Initialize analyzer with custom parameters\n",
    "    analyzer = NeuralSpectralAnalyzer(\n",
    "        base_path=base_directory,\n",
    "        output_subdir=\"oscillation_analysis\",\n",
    "        bin_size_ms=2.0,           # 2ms temporal resolution\n",
    "        kernel_window_ms=300,      # 300ms smoothing window\n",
    "        tau_rise_ms=2.0,          # Fast synaptic rise\n",
    "        tau_decay_ms=25.0         # Slower synaptic decay\n",
    "    )\n",
    "    \n",
    "    # Process all JSON files in nested folder structure\n",
    "    if json_input_folder.exists():\n",
    "        results = analyzer.process_nested_folders(json_input_folder)\n",
    "        print(f\"Processing complete. Results: {results}\")\n",
    "    else:\n",
    "        print(f\"Input folder {json_input_folder} does not exist. Please update the path.\")\n",
    "        \n",
    "    # Alternative: Process a single folder\n",
    "    # single_folder = json_input_folder / \"experimental_condition_1\"\n",
    "    # if single_folder.exists():\n",
    "    #     stats = analyzer.process_folder(single_folder)\n",
    "    #     print(f\"Processed {stats['processed']} files, {stats['failed']} failed\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "team_ndd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
